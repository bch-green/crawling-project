"""
1c.py - ì„ìƒì‹œí—˜ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ (ìƒì„¸ í˜ì´ì§€ ë°©ì‹, ìµœì¢…ë³¸)
- ì—°ë„ë³„ ì •ìˆœ(ì˜¤ë¦„ì°¨ìˆœ) í¬ë¡¤ë§: 2019 -> 2020 -> ... -> 2024
- ì—°ì† ë¯¸ì¡´ì¬ ì„ê³„ì¹˜ 10íšŒë¡œ ì—°ë„ ì¢…ë£Œ ìë™ íŒë‹¨ (ë¹ ë¥¸ ì¢…ë£Œ)
- 2019~2024ëŠ” 'ê¸°ì¡´ì— ìˆì–´ë„' ì¬ìˆ˜ì§‘, ê·¸ ì™¸ ì—°ë„ëŠ” ê¸°ì¡´ ë°ì´í„° ìŠ¤í‚µ
- ë„ë©”ì¸ ìë™ ê°ì§€ (trialforme.konect.or.kr / koreaclinicaltrials.org)
- URL ì •ê·œí™”(ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±°) ë° ë¦¬ë‹¤ì´ë ‰íŠ¸ ë² ì´ìŠ¤ ê°±ì‹ 
- ë ˆê±°ì‹œ(2019~2024) + ì‹ ê·œ(2025) í…œí”Œë¦¿ ëª¨ë‘ ëŒ€ì‘(txt-group/table/dl)
"""

import os
import re
import time
from datetime import datetime
from typing import Optional, Set, Iterable
from urllib.parse import urlparse, urlunparse

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# =========================================================
# í¬ë¡¤ëŸ¬
# =========================================================
class ClinicalTrialCrawler:
    def __init__(self):
        # ì‹¤ì œ ì„œë¹„ìŠ¤ê°€ ì œê³µë˜ëŠ” í›„ë³´ ë„ë©”ì¸ (ìš°ì„ ìˆœìœ„)
        self.candidate_bases = [
            "https://trialforme.konect.or.kr",
            "https://www.koreaclinicaltrials.org",
        ]
        # ì´ˆê¸° ë² ì´ìŠ¤ (ê°ì§€ í›„ ê°±ì‹ ë¨)
        self.base_url = self.candidate_bases[0]
        self.driver = None
        self.all_data = []
        self.csv_path = 'clinical_trials_full.csv'
        # ë„ë©”ì¸ ê°ì§€ìš© í”„ë¡œë¸Œ SN (ì˜ˆì‹œ)
        self.probe_sn = 202499968

    # -------------------------------
    # ê³µí†µ ìœ í‹¸
    # -------------------------------
    def _normalize_path(self, path: str) -> str:
        """ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±°: //clnctest â†’ /clnctest"""
        return re.sub(r'/{2,}', '/', path)

    def build_url(self, base: str, path: str, query: str = "") -> str:
        """base + path + queryë¡œ ì•ˆì „í•˜ê²Œ URL ìƒì„±"""
        path = self._normalize_path(path)
        if query and not query.startswith("?"):
            query = "?" + query
        parts = urlparse(base)
        return urlunparse((parts.scheme, parts.netloc, path, "", query.lstrip("?"), ""))

    def sn_to_year(self, sn: int) -> int:
        """SN ì• 4ìë¦¬ë¥¼ ì—°ë„ë¡œ í•´ì„ (ì˜ˆ: 201900001 -> 2019)"""
        try:
            return int(str(sn)[:4])
        except Exception:
            return -1

    # -------------------------------
    # ë“œë¼ì´ë²„/ë„ë©”ì¸
    # -------------------------------
    def setup_driver(self, headless: bool = False):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì • + ë„ë©”ì¸ ìë™ ê°ì§€"""
        options = Options()
        if headless:
            options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--window-size=1280,1600")
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        )
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_page_load_timeout(30)
        print("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")

        # ë„ë©”ì¸ ìë™ ê°ì§€
        self.detect_and_set_base_url()

    def detect_and_set_base_url(self, timeout: int = 8) -> None:
        """í›„ë³´ ë„ë©”ì¸ì„ ìˆœì°¨ ì‹œë„í•´ ì‹¤ì œ ë™ì‘í•˜ëŠ” base_urlë¡œ ê°±ì‹ """
        for cand in self.candidate_bases:
            try:
                url = self.build_url(cand, "/clnctest/view.do", f"clncTestSn={self.probe_sn}")
                self.driver.get(url)
                WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                # ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€ ì‹œ í˜„ì¬ í˜¸ìŠ¤íŠ¸ë¡œ êµì²´
                cur = urlparse(self.driver.current_url)
                detected = f"{cur.scheme}://{cur.netloc}"
                self.base_url = detected
                print(f"ğŸŒ ë² ì´ìŠ¤ ë„ë©”ì¸ ì„¤ì •: {self.base_url}")
                return
            except Exception:
                continue
        print("âš ï¸ ë² ì´ìŠ¤ ë„ë©”ì¸ ìë™ ê°ì§€ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©:", self.base_url)

    # -------------------------------
    # ê°€ë¹„ì§€ íƒ€ì´í‹€ ê°ì§€
    # -------------------------------
    def _looks_garbage_page(self, text: str) -> bool:
        """ë ˆì´ì•„ì›ƒ/ëª©ë¡ í…ìŠ¤íŠ¸ê°€ ì œëª©ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°ë¥¼ ë¯¸ì¡´ì¬ë¡œ ê°„ì£¼"""
        if not text:
            return True
        t = text.replace(",", " ").strip()
        bad_keys = [
            "ì„ìƒì‹œí—˜ ì •ë³´", "ì‹ì•½ì²˜ ìŠ¹ì¸ ëª©ë¡", "ëª©ë¡ìœ¼ë¡œ", "ì˜ì•½í’ˆ ì •ë³´",
            "ì‹¤ì‹œê¸°ê´€ ì •ë³´", "ëŒ€ìƒì ì„ ì •ê¸°ì¤€", "ëŒ€ìƒì ì œì™¸ê¸°ì¤€",
            "ì—°êµ¬ì„¤ê³„ ë° ìˆ˜í–‰ë°©ë²•", "ìµœì´ˆ ì‚¬ëŒëŒ€ìƒ ì—°êµ¬ì—¬ë¶€"
        ]
        hit = sum(1 for k in bad_keys if k in t)
        if hit >= 2:
            return True
        if len(t) > 300:  # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ë¤í”„
            return True
        return False

    # -------------------------------
    # ìƒì„¸ í˜ì´ì§€ íŒŒì„œ (ë‹¤ì¤‘ í…œí”Œë¦¿)
    # -------------------------------
    def extract_detail_data(self, clnc_test_sn: int, wait_sec: int = 8) -> Optional[dict]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ (2019~2025 ëª¨ë“  í…œí”Œë¦¿ ëŒ€ì‘)"""
        url = self.build_url(self.base_url, "/clnctest/view.do", f"clncTestSn={clnc_test_sn}")
        try:
            self.driver.get(url)
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‹œ base ê°±ì‹ 
            cur = urlparse(self.driver.current_url)
            cur_base = f"{cur.scheme}://{cur.netloc}"
            if cur_base != self.base_url:
                print(f"ğŸ” ë„ë©”ì¸ ë³€ê²½ ê°ì§€: {self.base_url} â†’ {cur_base}")
                self.base_url = cur_base
        except Exception as e:
            print(f"âŒ {clnc_test_sn} í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        try:
            WebDriverWait(self.driver, wait_sec).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print(f"âš ï¸ {clnc_test_sn} body ë¡œë“œ íƒ€ì„ì•„ì›ƒ")
            return None

        data = {
            "clncTestSn": str(clnc_test_sn),
            "ì§„í–‰ìƒíƒœ": "",
            "í¬ë¡¤ë§ì¼ì‹œ": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # 1) ì„ìƒì‹œí—˜ëª… (í•„ìˆ˜)
        title = self._find_first_text([
            (By.CSS_SELECTOR, "div.recruit-group2 > div.box"),  # 2025
            (By.CSS_SELECTOR, "div.recruit-group2 .box .tit, div.recruit-group2 .box .title"),
            (By.CSS_SELECTOR, "div.recruit-detail h3"),
            (By.CSS_SELECTOR, "div.view-tit, .view-tit, .view_title"),
            (By.CSS_SELECTOR, "h2.tit, h3.tit, h1.tit"),
            (By.CSS_SELECTOR, "div.recruit-detail, #contents, .contents, .container")
        ])
        if not title or self._looks_garbage_page(title):
            # ì œëª©ì´ ì—†ê±°ë‚˜ ë ˆì´ì•„ì›ƒ ë¤í”„ë©´ ë¯¸ì¡´ì¬ë¡œ ì²˜ë¦¬
            # print(f"âš ï¸ {clnc_test_sn} ë¹„ì •ìƒ/ê°€ë¹„ì§€ í˜ì´ì§€ë¡œ íŒë‹¨ â†’ ë¯¸ì¡´ì¬")
            return None
        data["ì„ìƒì‹œí—˜ëª…"] = title

        # 2) ìƒì„¸ ì •ë³´ (txt-group / table / dl ëª¨ë‘ ì‹œë„)
        self._extract_from_txt_groups(data, [
            "div.recruit-detail div.txt-group",
            "div.txt-group",
            "section.detail .txt-group",
        ])
        self._extract_from_tables(data, [
            "table.view, table.tbl-view, table.tbl, table.table, .view table, .tbl table",
            "div.recruit-detail table",
            "table"
        ])
        self._extract_from_definition_lists(data, [
            "dl.view, dl.list, dl.info, .view dl, .info dl, dl"
        ])

        # 3) ì‹¤ì‹œê¸°ê´€ (íƒ­)
        self._open_institution_tab()
        self._extract_institutions(data)

        return data

    # -------------------------------
    # ì—°ë„ë³„ ì •ìˆœ í¬ë¡¤ë§
    # -------------------------------
    def crawl_years(
        self,
        years: Iterable[int],
        headless: bool = False,
        refresh_years: Optional[Set[int]] = None,
        year_missing_threshold: int = 10,   # ğŸ”½ 10ìœ¼ë¡œ ë‚®ì¶¤
    ) -> None:
        """
        ì—°ë„ë³„ë¡œ ì‹œì‘ SN(YYYY00001)ë¶€í„° 1ì”© ì¦ê°€í•˜ë©° ìˆ˜ì§‘.
        ë¯¸ì¡´ì¬ SNì´ ì—°ì†ìœ¼ë¡œ year_missing_threshold íšŒ ë‚˜ì˜¤ë©´ í•´ë‹¹ ì—°ë„ ì¢…ë£Œ.
        - refresh_yearsì— í¬í•¨ëœ ì—°ë„ëŠ” ê¸°ì¡´ CSVì— ìˆì–´ë„ ì¬ìˆ˜ì§‘.
        """
        try:
            self.setup_driver(headless=headless)

            # ì •ê·œí™”
            years = list(sorted(set(years)))
            if refresh_years is not None and not isinstance(refresh_years, set):
                refresh_years = set(refresh_years)

            print(f"ğŸ“… ëŒ€ìƒ ì—°ë„: {years}")
            if refresh_years:
                print(f"ğŸ” ì¬ìˆ˜ì§‘ ì—°ë„: {sorted(refresh_years)}")
            print("=" * 50)

            # ê¸°ì¡´ CSV ì¤‘ë³µ íšŒí”¼ ì§‘í•©
            existing_sns: Set[str] = set()
            if os.path.exists(self.csv_path):
                existing_df = pd.read_csv(self.csv_path, dtype={'clncTestSn': str})
                existing_sns = set(existing_df['clncTestSn'].astype(str).values)
                print(f"ğŸ“‚ ê¸°ì¡´ íŒŒì¼ì—ì„œ {len(existing_sns)}ê°œ í•­ëª© í™•ì¸")

            success_count = fail_count = skip_count = 0
            processed_in_year = 0

            for year in years:
                start_sn = year * 100000 + 1
                end_sn_exclusive = (year + 1) * 100000  # ì´ ê°’ì€ í¬í•¨í•˜ì§€ ì•ŠìŒ
                print(f"\nğŸ—“ï¸ {year}ë…„ í¬ë¡¤ë§ ì‹œì‘: SN {start_sn} â†’ < {end_sn_exclusive} (ì •ìˆœ)")
                consecutive_missing = 0
                processed_in_year = 0

                sn = start_sn
                while sn < end_sn_exclusive:
                    # ìŠ¤í‚µ íŒë‹¨
                    if not (refresh_years and year in refresh_years) and (str(sn) in existing_sns):
                        skip_count += 1
                        if skip_count % 200 == 0:
                            print(f"â­ï¸ ëˆ„ì  ìŠ¤í‚µ {skip_count}ê°œ (ìµœê·¼ ìŠ¤í‚µ SN: {sn})")
                        sn += 1
                        continue

                    # ì§„í–‰ ë¡œê·¸(ê³¼ë‹¤ ì¶œë ¥ ë°©ì§€)
                    if processed_in_year % 100 == 0:
                        print(f"[{year}] ì§„í–‰ SN: {sn} (ì—°ì† ë¯¸ì¡´ì¬: {consecutive_missing}/{year_missing_threshold})")

                    data = self.extract_detail_data(sn)

                    if data:
                        self.all_data.append(data)
                        success_count += 1
                        processed_in_year += 1
                        consecutive_missing = 0
                        # 100ê°œë§ˆë‹¤ ë°±ì—…
                        if success_count % 100 == 0:
                            self.save_backup(success_count)
                    else:
                        fail_count += 1
                        processed_in_year += 1
                        consecutive_missing += 1

                        # ì—°ì† ë¯¸ì¡´ì¬ ì„ê³„ ë„ë‹¬ â†’ í•´ë‹¹ ì—°ë„ ì¢…ë£Œ
                        if consecutive_missing >= year_missing_threshold:
                            print(f"ğŸ§­ {year}ë…„ ì¢…ë£Œ ì¶”ì •: ì—°ì† ë¯¸ì¡´ì¬ {consecutive_missing}íšŒ (ë§ˆì§€ë§‰ ì‹œë„ SN: {sn})")
                            break

                        # ì•ˆì •ì„±: 10íšŒ ì—°ì† ì‹¤íŒ¨ë§ˆë‹¤ ë“œë¼ì´ë²„ ë¦¬ì…‹
                        if consecutive_missing % 10 == 0:
                            print("ğŸ”§ ì•ˆì •í™”: ë“œë¼ì´ë²„ ì¬ì‹œì‘ (ì—°ì† ì‹¤íŒ¨ ëˆ„ì )")
                            try:
                                self.driver.quit()
                            except Exception:
                                pass
                            time.sleep(2)
                            self.setup_driver(headless=headless)

                    # 500ê°œ ì²˜ë¦¬ë§ˆë‹¤ ë“œë¼ì´ë²„ ì¬ì‹œì‘(ë©”ëª¨ë¦¬ ê´€ë¦¬)
                    if processed_in_year > 0 and processed_in_year % 500 == 0:
                        print("ğŸ§¹ 500ê°œ ì²˜ë¦¬ - ë“œë¼ì´ë²„ ì¬ì‹œì‘(ë©”ëª¨ë¦¬ ì •ë¦¬)")
                        try:
                            self.driver.quit()
                        except Exception:
                            pass
                        time.sleep(3)
                        self.setup_driver(headless=headless)

                    # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    import random
                    time.sleep(random.uniform(1.5, 4.0))

                    sn += 1

                # ì—°ë„ ìš”ì•½
                print(f"âœ… {year}ë…„ ì™„ë£Œ(ì¶”ì •) | ëˆ„ì  ì„±ê³µ:{success_count}, ì‹¤íŒ¨:{fail_count}, ìŠ¤í‚µ:{skip_count}")

            # ì „ì²´ ìš”ì•½
            print("\n" + "=" * 50)
            print("ğŸ“Š ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")
            print(f"  - ì´ ì„±ê³µ: {success_count}")
            print(f"  - ì´ ì‹¤íŒ¨: {fail_count}")
            print(f"  - ì´ ìŠ¤í‚µ: {skip_count}")

        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            if self.all_data:
                try:
                    self.save_final()
                except Exception as e:
                    print(f"âŒ ìµœì¢… ì €ì¥ ì‹¤íŒ¨: {e}")
            if self.driver:
                try:
                    self.driver.quit()
                except Exception:
                    pass
                print("ğŸ”š ë“œë¼ì´ë²„ ì¢…ë£Œ")

    # -------------------------------
    # ì €ì¥/ê²€ì¦
    # -------------------------------
    def save_backup(self, count: int) -> None:
        """ì¤‘ê°„ ë°±ì—… ì €ì¥"""
        backup_file = f"backup_1c_{count}.csv"
        df = pd.DataFrame(self.all_data)
        df.to_csv(backup_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ë°±ì—… ì €ì¥: {backup_file}")

    def save_final(self) -> Optional[str]:
        """ìµœì¢… CSV ì €ì¥ (ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©)"""
        if not self.all_data:
            print("âš ï¸ ì €ì¥í•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None

        new_df = pd.DataFrame(self.all_data)
        if os.path.exists(self.csv_path):
            existing_df = pd.read_csv(self.csv_path, dtype={'clncTestSn': str})
            combined_df = pd.concat([new_df, existing_df], ignore_index=True)
        else:
            combined_df = new_df

        combined_df.drop_duplicates(subset='clncTestSn', keep='first', inplace=True)
        combined_df['clncTestSn_int'] = pd.to_numeric(combined_df['clncTestSn'], errors='coerce')
        combined_df.sort_values(by='clncTestSn_int', ascending=True, inplace=True, na_position='last')  # ì •ìˆœ ì €ì¥
        combined_df.drop(columns=['clncTestSn_int'], inplace=True)

        combined_df.to_csv(self.csv_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ìµœì¢… ì €ì¥ ì™„ë£Œ: {self.csv_path}")
        print(f"ğŸ“Š ì´ {len(combined_df)}ê°œ í•­ëª©")

        try:
            max_sn = pd.to_numeric(combined_df['clncTestSn'], errors='coerce').max()
            if pd.notna(max_sn):
                with open('last_clnc_test_sn.txt', 'w') as f:
                    f.write(str(int(max_sn)))
                print(f"ğŸ”¢ ìµœëŒ€ clncTestSn: {int(max_sn)} (last_clnc_test_sn.txtì— ì €ì¥)")
        except Exception as e:
            print(f"âš ï¸ ìµœëŒ€ SN ì €ì¥ ì‹¤íŒ¨: {e}")

        return self.csv_path

    def verify_csv(self, filename: str) -> bool:
        """CSV íŒŒì¼ ê²€ì¦"""
        try:
            df = pd.read_csv(filename, dtype={'clncTestSn': str})
            print("\nğŸ“‹ CSV ê²€ì¦ ê²°ê³¼:")
            print(f"  - ì´ í–‰ ìˆ˜: {len(df)}")
            print(f"  - ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
            print(f"  - ì²« 5í–‰ clncTestSn: {', '.join(df['clncTestSn'].head(5))}")
            return True
        except Exception as e:
            print(f"âŒ CSV ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    # -------------------------------
    # íŒŒì‹± í—¬í¼ë“¤
    # -------------------------------
    def _get_text_safe(self, el) -> str:
        try:
            t = el.text.strip()
            return re.sub(r"\s+", " ", t)
        except Exception:
            return ""

    def _find_first_text(self, candidates) -> str:
        for by, sel in candidates:
            try:
                el = self.driver.find_element(by, sel)
                text = self._get_text_safe(el)
                if text:
                    if len(text) > 500 and "\n" in text:
                        first_line = text.split("\n", 1)[0].strip()
                        if first_line:
                            return first_line
                    return text
            except NoSuchElementException:
                continue
            except Exception:
                continue
        return ""

    def _extract_from_txt_groups(self, data: dict, groups_selectors) -> None:
        for sel in groups_selectors:
            try:
                groups = self.driver.find_elements(By.CSS_SELECTOR, sel)
                for g in groups:
                    try:
                        key_el = None
                        val_el = None
                        for ksel in [".tit", ".title", "strong", "b", "h4", "h5"]:
                            try:
                                key_el = g.find_element(By.CSS_SELECTOR, ksel)
                                if key_el:
                                    break
                            except NoSuchElementException:
                                continue
                        for vsel in [".txt", ".desc", ".cont", "p", "div"]:
                            try:
                                val_el = g.find_element(By.CSS_SELECTOR, vsel)
                                if val_el:
                                    break
                            except NoSuchElementException:
                                continue

                        key = self._get_text_safe(key_el) if key_el else ""
                        val = self._get_text_safe(val_el) if val_el else ""
                        if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                            data[key] = val
                    except Exception:
                        continue
            except Exception:
                continue

    def _extract_from_tables(self, data: dict, table_selectors) -> None:
        for sel in table_selectors:
            try:
                tables = self.driver.find_elements(By.CSS_SELECTOR, sel)
            except Exception:
                continue
            for tb in tables:
                try:
                    rows = tb.find_elements(By.CSS_SELECTOR, "tr")
                    for r in rows:
                        ths = r.find_elements(By.CSS_SELECTOR, "th")
                        tds = r.find_elements(By.CSS_SELECTOR, "td")
                        if ths and tds:
                            pairs = min(len(ths), len(tds))
                            for i in range(pairs):
                                key = self._get_text_safe(ths[i])
                                val = self._get_text_safe(tds[i])
                                if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                                    data[key] = val
                except Exception:
                    continue

    def _extract_from_definition_lists(self, data: dict, dl_selectors) -> None:
        for sel in dl_selectors:
            try:
                dls = self.driver.find_elements(By.CSS_SELECTOR, sel)
            except Exception:
                continue
            for dl in dls:
                try:
                    dts = dl.find_elements(By.TAG_NAME, "dt")
                    dds = dl.find_elements(By.TAG_NAME, "dd")
                    pairs = min(len(dts), len(dds))
                    for i in range(pairs):
                        key = self._get_text_safe(dts[i])
                        val = self._get_text_safe(dds[i])
                        if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                            data[key] = val
                except Exception:
                    continue

    def _open_institution_tab(self) -> None:
        """'ì‹¤ì‹œê¸°ê´€' íƒ­ì„ ì—¬ëŠ” ì•ˆì „í•œ ë°©ë²•: í´ë¦­ â†’ ì‹¤íŒ¨ ì‹œ JS ê°•ì œ ë…¸ì¶œ"""
        try:
            tab_btn = None
            for xp in [
                "//a[contains(., 'ì‹¤ì‹œê¸°ê´€')]",
                "//button[contains(., 'ì‹¤ì‹œê¸°ê´€')]",
                "//li[a[contains(., 'ì‹¤ì‹œê¸°ê´€')]]/a",
            ]:
                try:
                    tab_btn = self.driver.find_element(By.XPATH, xp)
                    if tab_btn:
                        break
                except NoSuchElementException:
                    continue
            if tab_btn:
                self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", tab_btn)
                tab_btn.click()
                try:
                    WebDriverWait(self.driver, 2).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "#tab2, #tab02"))
                    )
                except TimeoutException:
                    pass
                return
        except Exception:
            pass

        # ê°•ì œ í‘œì‹œ
        for sel in ["#tab2", "#tab02"]:
            try:
                el = self.driver.find_element(By.CSS_SELECTOR, sel)
                self.driver.execute_script("arguments[0].style.display='block';", el)
                return
            except Exception:
                continue

    def _extract_institutions(self, data: dict, max_rows: int = 30) -> None:
        """í…Œì´ë¸” ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì‹œê¸°ê´€/ë‹´ë‹¹ì í˜•íƒœë¥¼ ìµœëŒ€ì¹˜ê¹Œì§€ ìˆ˜ì§‘"""
        containers = []
        for sel in ["#tab2", "#tab02", "section.institution, .institution, .org-list"]:
            try:
                containers.extend(self.driver.find_elements(By.CSS_SELECTOR, sel))
            except Exception:
                continue

        def _scan_tables(root, start_idx=1):
            idx = start_idx
            tables = root.find_elements(By.TAG_NAME, "table")
            for tb in tables:
                try:
                    rows = tb.find_elements(By.CSS_SELECTOR, "tbody tr") or tb.find_elements(By.CSS_SELECTOR, "tr")
                    for r in rows:
                        cols = r.find_elements(By.TAG_NAME, "td")
                        if not cols:
                            continue
                        name = self._get_text_safe(cols[0])
                        if not name:
                            continue
                        data[f"ì‹¤ì‹œê¸°ê´€{idx}"] = name
                        if len(cols) > 1:
                            data[f"ì‹¤ì‹œê¸°ê´€{idx}_ë‹´ë‹¹ì"] = self._get_text_safe(cols[1])
                        if len(cols) > 2:
                            extra = [self._get_text_safe(c) for c in cols[2:]]
                            extra = [x for x in extra if x]
                            if extra:
                                data[f"ì‹¤ì‹œê¸°ê´€{idx}_ê¸°íƒ€"] = " | ".join(extra)
                        idx += 1
                        if idx > start_idx + max_rows - 1:
                            return idx
                except Exception:
                    continue
            return idx

        next_idx = 1
        for c in containers:
            next_idx = _scan_tables(c, next_idx)
            if next_idx > max_rows:
                return

        # captionì— 'ì‹¤ì‹œê¸°ê´€' í¬í•¨ í…Œì´ë¸” ë°±ì—… ìŠ¤ìº”
        try:
            tables = self.driver.find_elements(By.TAG_NAME, "table")
            for tb in tables:
                cap_text = ""
                try:
                    cap_text = tb.find_element(By.TAG_NAME, "caption").text
                except Exception:
                    pass
                if "ì‹¤ì‹œê¸°ê´€" in cap_text:
                    next_idx = _scan_tables(tb, next_idx)
                    if next_idx > max_rows:
                        return
        except Exception:
            pass


# =========================================================
# ë©”ì¸
# =========================================================
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì„ìƒì‹œí—˜ ì—°ë„ë³„ ì •ìˆœ í¬ë¡¤ë§ ì‹œì‘")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    crawler = ClinicalTrialCrawler()

    try:
        # âœ… 2019~2024 ì—°ë„ë§Œ ì •ìˆœìœ¼ë¡œ ìˆ˜ì§‘ (í•´ë‹¹ ì—°ë„ëŠ” ê¸°ì¡´ CSVì— ìˆì–´ë„ ì¬ìˆ˜ì§‘)
        years = range(2021, 2023)          # 2019,2020,...,2024
        refresh = set(range(2021, 2023))   # ì¬ìˆ˜ì§‘ ëŒ€ìƒ ì—°ë„
        # ìš´ì˜ ì‹œ headless=True ê¶Œì¥, ë¯¸ì¡´ì¬ ì„ê³„ì¹˜ëŠ” 10 (í•„ìš”ì‹œ íŠœë‹)
        crawler.crawl_years(
            years=years,
            headless=False,
            refresh_years=refresh,
            year_missing_threshold=10
        )

        # ê²€ì¦
        if os.path.exists(crawler.csv_path):
            crawler.verify_csv(crawler.csv_path)

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        if crawler.all_data:
            crawler.save_final()


if __name__ == "__main__":
    main()
