#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„ìƒì‹œí—˜ ì¦ë¶„ í¬ë¡¤ë§ ë„êµ¬ (2c.py)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë§¤ì¼ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ ìƒˆë¡œìš´ ì„ìƒì‹œí—˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘í•˜ëŠ” ì¦ë¶„ í¬ë¡¤ëŸ¬ì…ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Google Sheetsì—ì„œ ìµœëŒ€ clncTestSnì„ ìë™ ê°ì§€
- ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì´í›„ ìƒˆë¡œìš´ ì„ìƒì‹œí—˜ë§Œ ì¦ë¶„ ìˆ˜ì§‘
- ë‘ ê°œ ì‚¬ì´íŠ¸ ìë™ ê°ì§€ ë° ìŠ¤ìœ„ì¹­
- 1c_fixed.pyì™€ ë™ì¼í•œ ë°ì´í„° ì¶”ì¶œ ë¡œì§ ì‚¬ìš©
- ì§€ìˆ˜ ë°±ì˜¤í”„ ë° ì—ëŸ¬ ë³µêµ¬ ì§€ì›

ì§€ì› ì‚¬ì´íŠ¸:
- trialforme.konect.or.kr (ìš°ì„ ìˆœìœ„)
- koreaclinicaltrials.org (ëŒ€ì²´ ì‚¬ì´íŠ¸)

ì‚¬ìš©ë²•:
  python crawler/2c.py --cfg config/settings.yaml

ì‘ì„±ì: ìë™í™” ì‹œìŠ¤í…œ
ìµœì¢… ìˆ˜ì •: 2025-09-12
"""

import os
import re
import time
import random
import argparse
from datetime import datetime
from typing import Optional, Set, List
from urllib.parse import urlparse, urlunparse

import pandas as pd
import yaml
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class IncrementalClinicalTrialCrawler:
    """
    ì„ìƒì‹œí—˜ ì¦ë¶„ í¬ë¡¤ë§ í´ë˜ìŠ¤
    
    Google Sheetsì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘ëœ clncTestSnì„ í™•ì¸í•˜ê³ ,
    ê·¸ ì´í›„ì˜ ìƒˆë¡œìš´ ì„ìƒì‹œí—˜ ë°ì´í„°ë§Œ ì¦ë¶„ ìˆ˜ì§‘í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ì£¼ìš” ë©”ì†Œë“œ:
    - get_max_clnc_sn_from_sheet(): Google Sheetsì—ì„œ ìµœëŒ€ SN ì¡°íšŒ
    - detect_working_domain(): ì‚¬ìš© ê°€ëŠ¥í•œ ë„ë©”ì¸ ê°ì§€
    - crawl_incremental(): ì¦ë¶„ í¬ë¡¤ë§ ì‹¤í–‰
    - extract_detail_data(): ìƒì„¸ ë°ì´í„° ì¶”ì¶œ (1c_fixed.pyì™€ ë™ì¼)
    """
    def __init__(self, config_path: str = "config/settings.yaml"):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            config_path (str): YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.load_config(config_path)
        
        # ì§€ì›ë˜ëŠ” ì‚¬ì´íŠ¸ ëª©ë¡ (ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬)
        self.candidate_bases = [
            "https://trialforme.konect.or.kr",    # ì£¼ ì‚¬ì´íŠ¸
            "https://www.koreaclinicaltrials.org",  # ëŒ€ì²´ ì‚¬ì´íŠ¸
        ]
        
        self.base_url = self.candidate_bases[0]  # ì´ˆê¸° ê¸°ë³¸ URL
        self.driver = None                       # Selenium WebDriver ê°ì²´
        self.all_data = []                       # ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥
        self.probe_sn = 202499968               # ë„ë©”ì¸ ê°ì§€ìš© í…ŒìŠ¤íŠ¸ SN

    def load_config(self, config_path: str):
        """
        YAML ì„¤ì • íŒŒì¼ì—ì„œ í¬ë¡¤ë§ ì„¤ì • ê°’ë“¤ì„ ë¡œë“œ
        
        ì„¤ì • í•­ëª©:
        - sheet_id: Google Sheets ë¬¸ì„œ ID
        - worksheet: ì›Œí¬ì‹œíŠ¸ íƒ­ ì´ë¦„
        - service_account_json: ì¸ì¦ íŒŒì¼ ê²½ë¡œ
        - since_sn_buffer: SN ë²„í¼ (ì•ˆì „ ë§ˆì§„)
        - output_dir: ì¶œë ¥ íŒŒì¼ ë””ë ‰í„°ë¦¬
        - wait_seconds: í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„
        - pause: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„
        - max_consecutive_miss: ì—°ì† ì‹¤íŒ¨ í—ˆìš© íšŸìˆ˜
        - url_templates: í¬ë¡¤ë§ ëŒ€ìƒ URL í…œí”Œë¦¿ ëª©ë¡
        
        Args:
            config_path (str): YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        
        # Google Sheets ì„¤ì •
        self.sheet_id = config.get("sheet_id", "")                    # Sheets ë¬¸ì„œ ID
        self.worksheet = config.get("worksheet", "")                  # ì›Œí¬ì‹œíŠ¸ íƒ­ ì´ë¦„
        self.service_account_json = config.get("service_account_json", "")  # ì¸ì¦ íŒŒì¼
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.since_sn_buffer = config.get("since_sn_buffer", 10)      # SN ë²„í¼ (ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•´)
        self.output_dir = config.get("output_dir", "outputs")         # ì¶œë ¥ ë””ë ‰í„°ë¦¬
        self.wait_seconds = config.get("wait_seconds", 8)            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°ì‹œê°„
        self.pause = config.get("pause", 0.35)                       # ìš”ì²­ ê°„ ì§€ì—°
        self.max_consecutive_miss = config.get("max_consecutive_miss", 20)  # ì—°ì† ì‹¤íŒ¨ í—ˆìš©
        # URL í…œí”Œë¦¿ ëª©ë¡ (ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ, ê¸°ë³¸ê°’ ì œê³µ)
        self.url_templates = config.get("url_templates", [
            "https://trialforme.konect.or.kr/clnctest/view.do?pageNo=&clncTestSn={sn}&relatedSearchKeyword=&searchText=&recruitStartDate=&recruitEndDate=&status=",
            "https://www.koreaclinicaltrials.org/clnctest/view.do?pageNo=&clncTestSn={sn}&relatedSearchKeyword=&searchText=&recruitStartDate=&recruitEndDate=&status="
        ])

    def get_max_sn_from_sheet(self) -> Optional[int]:
        """Google Sheetsì—ì„œ ìµœëŒ€ clncTestSn ì¡°íšŒ"""
        try:
            import gspread
            from google.oauth2.service_account import Credentials

            if not os.path.exists(self.service_account_json):
                print(f"âš ï¸ ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.service_account_json}")
                return None

            creds = Credentials.from_service_account_file(
                self.service_account_json,
                scopes=["https://www.googleapis.com/auth/spreadsheets"]
            )
            gc = gspread.authorize(creds)
            sh = gc.open_by_key(self.sheet_id)
            ws = sh.worksheet(self.worksheet)

            # í—¤ë”ì—ì„œ clncTestSn ì»¬ëŸ¼ ì°¾ê¸°
            header = ws.row_values(1)
            clnc_col_idx = None
            for i, col in enumerate(header):
                if col.strip().lower() == "clnctestsn":
                    clnc_col_idx = i + 1
                    break

            if clnc_col_idx is None:
                print("âš ï¸ clncTestSn ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

            # í•´ë‹¹ ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ ê°€ì ¸ì˜¤ê¸°
            values = ws.col_values(clnc_col_idx)[1:]  # í—¤ë” ì œì™¸
            sn_list = []
            for v in values:
                try:
                    sn_list.append(int(str(v).strip()))
                except:
                    pass

            return max(sn_list) if sn_list else None

        except Exception as e:
            print(f"âš ï¸ Google Sheets ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def _normalize_path(self, path: str) -> str:
        """ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì œê±°"""
        return re.sub(r'/{2,}', '/', path)

    def build_url(self, base: str, path: str, query: str = "") -> str:
        """URL ì•ˆì „ ìƒì„±"""
        path = self._normalize_path(path)
        if query and not query.startswith("?"):
            query = "?" + query
        parts = urlparse(base)
        return urlunparse((parts.scheme, parts.netloc, path, "", query.lstrip("?"), ""))

    def setup_driver(self, headless: bool = True):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        options = Options()
        if headless:
            options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument("--window-size=1280,1600")
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        )
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_page_load_timeout(30)
        print("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")

        # ë„ë©”ì¸ ìë™ ê°ì§€
        self.detect_and_set_base_url()

    def detect_and_set_base_url(self, timeout: int = 8) -> None:
        """ë™ì‘í•˜ëŠ” ë² ì´ìŠ¤ URL ê°ì§€"""
        for cand in self.candidate_bases:
            try:
                url = self.build_url(cand, "/clnctest/view.do", f"clncTestSn={self.probe_sn}")
                self.driver.get(url)
                WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                cur = urlparse(self.driver.current_url)
                detected = f"{cur.scheme}://{cur.netloc}"
                self.base_url = detected
                print(f"ğŸŒ ë² ì´ìŠ¤ ë„ë©”ì¸ ì„¤ì •: {self.base_url}")
                return
            except Exception:
                continue
        print("âš ï¸ ë² ì´ìŠ¤ ë„ë©”ì¸ ìë™ ê°ì§€ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©:", self.base_url)

    def _looks_garbage_page(self, text: str) -> bool:
        """ê°€ë¹„ì§€ í˜ì´ì§€ ê°ì§€ (1c_fixed.pyì™€ ë™ì¼)"""
        if not text:
            return True
        t = text.replace(",", " ").strip()
        bad_keys = [
            "ì„ìƒì‹œí—˜ ì •ë³´", "ì‹ì•½ì²˜ ìŠ¹ì¸ ëª©ë¡", "ëª©ë¡ìœ¼ë¡œ", "ì˜ì•½í’ˆ ì •ë³´",
            "ì‹¤ì‹œê¸°ê´€ ì •ë³´", "ëŒ€ìƒì ì„ ì •ê¸°ì¤€", "ëŒ€ìƒì ì œì™¸ê¸°ì¤€",
            "ì—°êµ¬ì„¤ê³„ ë° ìˆ˜í–‰ë°©ë²•", "ìµœì´ˆ ì‚¬ëŒëŒ€ìƒ ì—°êµ¬ì—¬ë¶€"
        ]
        hit = sum(1 for k in bad_keys if k in t)
        if hit >= 2:
            return True
        if len(t) > 300:
            return True
        return False

    def extract_detail_data(self, clnc_test_sn: int, wait_sec: int = 8) -> Optional[dict]:
        """
        ìƒì„¸ í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ (1c_fixed.pyì˜ extract_detail_dataì™€ ë™ì¼)
        """
        url = self.build_url(self.base_url, "/clnctest/view.do", f"clncTestSn={clnc_test_sn}")
        try:
            self.driver.get(url)
            cur = urlparse(self.driver.current_url)
            cur_base = f"{cur.scheme}://{cur.netloc}"
            if cur_base != self.base_url:
                self.base_url = cur_base
        except Exception as e:
            print(f"âŒ {clnc_test_sn} í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

        try:
            WebDriverWait(self.driver, wait_sec).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException:
            print(f"âš ï¸ {clnc_test_sn} body ë¡œë“œ íƒ€ì„ì•„ì›ƒ")
            return None

        data = {
            "clncTestSn": str(clnc_test_sn),
            "ì§„í–‰ìƒíƒœ": "",
            "í¬ë¡¤ë§ì¼ì‹œ": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # 1) ì„ìƒì‹œí—˜ëª… ì¶”ì¶œ
        title = self._find_first_text([
            (By.CSS_SELECTOR, "div.recruit-group2 > div.box"),
            (By.CSS_SELECTOR, "div.recruit-group2 .box .tit, div.recruit-group2 .box .title"),
            (By.CSS_SELECTOR, "div.recruit-detail h3"),
            (By.CSS_SELECTOR, "div.view-tit, .view-tit, .view_title"),
            (By.CSS_SELECTOR, "h2.tit, h3.tit, h1.tit"),
            (By.CSS_SELECTOR, "div.recruit-detail, #contents, .contents, .container")
        ])
        
        if not title or self._looks_garbage_page(title):
            return None
        data["ì„ìƒì‹œí—˜ëª…"] = title

        # 2) ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        self._extract_from_txt_groups(data, [
            "div.recruit-detail div.txt-group",
            "div.txt-group",
            "section.detail .txt-group",
        ])
        self._extract_from_tables(data, [
            "table.view, table.tbl-view, table.tbl, table.table, .view table, .tbl table",
            "div.recruit-detail table",
            "table"
        ])
        self._extract_from_definition_lists(data, [
            "dl.view, dl.list, dl.info, .view dl, .info dl, dl"
        ])

        # 3) ì‹¤ì‹œê¸°ê´€ ì¶”ì¶œ
        self._open_institution_tab()
        self._extract_institutions(data)

        return data

    def _get_text_safe(self, el) -> str:
        """ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            t = el.text.strip()
            return re.sub(r"\s+", " ", t)
        except Exception:
            return ""

    def _find_first_text(self, candidates) -> str:
        """ì²« ë²ˆì§¸ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        for by, sel in candidates:
            try:
                el = self.driver.find_element(by, sel)
                text = self._get_text_safe(el)
                if text:
                    if len(text) > 500 and "\n" in text:
                        first_line = text.split("\n", 1)[0].strip()
                        if first_line:
                            return first_line
                    return text
            except NoSuchElementException:
                continue
            except Exception:
                continue
        return ""

    def _extract_from_txt_groups(self, data: dict, groups_selectors):
        """txt-groupì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        for sel in groups_selectors:
            try:
                groups = self.driver.find_elements(By.CSS_SELECTOR, sel)
                for g in groups:
                    try:
                        key_el = None
                        val_el = None
                        for ksel in [".tit", ".title", "strong", "b", "h4", "h5"]:
                            try:
                                key_el = g.find_element(By.CSS_SELECTOR, ksel)
                                if key_el:
                                    break
                            except NoSuchElementException:
                                continue
                        for vsel in [".txt", ".desc", ".cont", "p", "div"]:
                            try:
                                val_el = g.find_element(By.CSS_SELECTOR, vsel)
                                if val_el:
                                    break
                            except NoSuchElementException:
                                continue

                        key = self._get_text_safe(key_el) if key_el else ""
                        val = self._get_text_safe(val_el) if val_el else ""
                        if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                            data[key] = val
                    except Exception:
                        continue
            except Exception:
                continue

    def _extract_from_tables(self, data: dict, table_selectors):
        """í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        for sel in table_selectors:
            try:
                tables = self.driver.find_elements(By.CSS_SELECTOR, sel)
            except Exception:
                continue
            for tb in tables:
                try:
                    rows = tb.find_elements(By.CSS_SELECTOR, "tr")
                    for r in rows:
                        ths = r.find_elements(By.CSS_SELECTOR, "th")
                        tds = r.find_elements(By.CSS_SELECTOR, "td")
                        if ths and tds:
                            pairs = min(len(ths), len(tds))
                            for i in range(pairs):
                                key = self._get_text_safe(ths[i])
                                val = self._get_text_safe(tds[i])
                                if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                                    data[key] = val
                except Exception:
                    continue

    def _extract_from_definition_lists(self, data: dict, dl_selectors):
        """ì •ì˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        for sel in dl_selectors:
            try:
                dls = self.driver.find_elements(By.CSS_SELECTOR, sel)
            except Exception:
                continue
            for dl in dls:
                try:
                    dts = dl.find_elements(By.TAG_NAME, "dt")
                    dds = dl.find_elements(By.TAG_NAME, "dd")
                    pairs = min(len(dts), len(dds))
                    for i in range(pairs):
                        key = self._get_text_safe(dts[i])
                        val = self._get_text_safe(dds[i])
                        if key and val and key not in ("ì„ìƒì‹œí—˜ëª…",):
                            data[key] = val
                except Exception:
                    continue

    def _open_institution_tab(self) -> None:
        """ì‹¤ì‹œê¸°ê´€ íƒ­ ì—´ê¸°"""
        try:
            tab_btn = None
            for xp in [
                "//a[contains(., 'ì‹¤ì‹œê¸°ê´€')]",
                "//button[contains(., 'ì‹¤ì‹œê¸°ê´€')]",
                "//li[a[contains(., 'ì‹¤ì‹œê¸°ê´€')]]/a",
            ]:
                try:
                    tab_btn = self.driver.find_element(By.XPATH, xp)
                    if tab_btn:
                        break
                except NoSuchElementException:
                    continue
            if tab_btn:
                self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", tab_btn)
                tab_btn.click()
                try:
                    WebDriverWait(self.driver, 2).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "#tab2, #tab02"))
                    )
                except TimeoutException:
                    pass
                return
        except Exception:
            pass

        # ê°•ì œ í‘œì‹œ
        for sel in ["#tab2", "#tab02"]:
            try:
                el = self.driver.find_element(By.CSS_SELECTOR, sel)
                self.driver.execute_script("arguments[0].style.display='block';", el)
                return
            except Exception:
                continue

    def _extract_institutions(self, data: dict, max_rows: int = 30) -> None:
        """ì‹¤ì‹œê¸°ê´€ ì •ë³´ ì¶”ì¶œ"""
        containers = []
        for sel in ["#tab2", "#tab02", "section.institution, .institution, .org-list"]:
            try:
                containers.extend(self.driver.find_elements(By.CSS_SELECTOR, sel))
            except Exception:
                continue

        def _scan_tables(root, start_idx=1):
            idx = start_idx
            tables = root.find_elements(By.TAG_NAME, "table")
            for tb in tables:
                try:
                    rows = tb.find_elements(By.CSS_SELECTOR, "tbody tr") or tb.find_elements(By.CSS_SELECTOR, "tr")
                    for r in rows:
                        cols = r.find_elements(By.TAG_NAME, "td")
                        if not cols:
                            continue
                        name = self._get_text_safe(cols[0])
                        if not name:
                            continue
                        data[f"ì‹¤ì‹œê¸°ê´€{idx}"] = name
                        if len(cols) > 1:
                            data[f"ì‹¤ì‹œê¸°ê´€{idx}_ë‹´ë‹¹ì"] = self._get_text_safe(cols[1])
                        if len(cols) > 2:
                            extra = [self._get_text_safe(c) for c in cols[2:]]
                            extra = [x for x in extra if x]
                            if extra:
                                data[f"ì‹¤ì‹œê¸°ê´€{idx}_ê¸°íƒ€"] = " | ".join(extra)
                        idx += 1
                        if idx > start_idx + max_rows - 1:
                            return idx
                except Exception:
                    continue
            return idx

        next_idx = 1
        for c in containers:
            next_idx = _scan_tables(c, next_idx)
            if next_idx > max_rows:
                return

    def crawl_incremental(self, since_sn: int, limit: Optional[int] = None, 
                         headless: bool = True, verbose: bool = False) -> List[dict]:
        """ì¦ë¶„ í¬ë¡¤ë§ ì‹¤í–‰"""
        
        rows = []
        misses = 0
        sn = since_sn + 1
        
        self.setup_driver(headless=headless)
        
        try:
            while True:
                if limit is not None and len(rows) >= limit:
                    break
                
                if verbose:
                    print(f"ğŸ” SN={sn} í¬ë¡¤ë§ ì‹œë„")
                
                data = self.extract_detail_data(sn)
                
                if data:
                    rows.append(data)
                    misses = 0
                    if verbose:
                        title_preview = data.get('ì„ìƒì‹œí—˜ëª…', '')[:50]
                        print(f"âœ… SN={sn} ìˆ˜ì§‘: {title_preview}...")
                else:
                    misses += 1
                    if verbose:
                        print(f"âŒ SN={sn} ë¯¸ì¡´ì¬ (ì—°ì†: {misses})")
                    
                    if misses >= self.max_consecutive_miss:
                        print(f"ğŸ›‘ ì—°ì† ë¯¸ì¡´ì¬ {misses}íšŒ â†’ í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                
                sn += 1
                time.sleep(self.pause + random.uniform(0, 0.2))
                
        finally:
            if self.driver:
                try:
                    self.driver.quit()
                except Exception:
                    pass
        
        return rows

    def save_to_csv(self, rows: List[dict], filename: str = None) -> str:
        """CSV ì €ì¥ (1c_fixed.pyì™€ ë™ì¼í•œ í˜•ì‹)"""
        if not rows:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"increment_{timestamp}.csv"
        
        output_path = os.path.join(self.output_dir, filename)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # 1c_fixed.pyì™€ ë™ì¼í•œ ì»¬ëŸ¼ ìˆœì„œ
        all_columns = set()
        for row in rows:
            all_columns.update(row.keys())
        
        # ê¸°ë³¸ ì»¬ëŸ¼ ìˆœì„œ
        base_columns = [
            "clncTestSn", "ì§„í–‰ìƒíƒœ", "í¬ë¡¤ë§ì¼ì‹œ", "ì„ìƒì‹œí—˜ëª…",
            "ì„ìƒì‹œí—˜ ì˜ë¢°ì", "ì†Œì¬ì§€", "ëŒ€ìƒì§ˆí™˜", "ëŒ€ìƒì§ˆí™˜ëª…",
            "ì„ìƒì‹œí—˜ ë‹¨ê³„", "ì„ìƒì‹œí—˜ ê¸°ê°„", "ì„±ë³„", "ë‚˜ì´",
            "ëª©í‘œ ëŒ€ìƒì ìˆ˜(êµ­ë‚´)", "ì„ìƒì‹œí—˜ ìŠ¹ì¸ì¼ì", "ìµœê·¼ ë³€ê²½ì¼ì", "ì´ìš©ë¬¸ì˜"
        ]
        
        # ì‹¤ì‹œê¸°ê´€ ì»¬ëŸ¼ë“¤
        institution_columns = []
        for i in range(1, 31):
            for suffix in ["", "_ë‹´ë‹¹ì", "_ê¸°íƒ€"]:
                col = f"ì‹¤ì‹œê¸°ê´€{i}{suffix}"
                if col in all_columns:
                    institution_columns.append(col)
        
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤
        remaining_columns = [col for col in sorted(all_columns) 
                           if col not in base_columns + institution_columns]
        
        final_columns = base_columns + institution_columns + remaining_columns
        
        # DataFrame ìƒì„± ë° ì €ì¥
        df = pd.DataFrame(rows)
        df = df.reindex(columns=final_columns, fill_value="")
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“Š {len(rows)}ê°œ í•­ëª©, {len(final_columns)}ê°œ ì»¬ëŸ¼")
        
        return output_path


def main():
    parser = argparse.ArgumentParser(description="ì¦ë¶„ ì„ìƒì‹œí—˜ í¬ë¡¤ëŸ¬ (1c_fixed.py í˜¸í™˜)")
    parser.add_argument("--cfg", default="config/settings.yaml", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--since-sn", type=int, help="ì‹œì‘ SN (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì‹œíŠ¸ì—ì„œ ìë™ ê³„ì‚°)")
    parser.add_argument("--limit", type=int, help="ìµœëŒ€ ìˆ˜ì§‘ ê±´ìˆ˜")
    parser.add_argument("--output", help="ì¶œë ¥ CSV íŒŒì¼ëª…")
    parser.add_argument("--headless", action="store_true", default=True, help="í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ë¡œê·¸")
    
    args = parser.parse_args()
    
    try:
        crawler = IncrementalClinicalTrialCrawler(args.cfg)
        
        # ì‹œì‘ SN ê²°ì •
        if args.since_sn is not None:
            since_sn = args.since_sn
            print(f"ğŸ“ ìˆ˜ë™ ì§€ì • since_sn: {since_sn}")
        else:
            print("ğŸ” Google Sheetsì—ì„œ ìµœëŒ€ SN ì¡°íšŒ ì¤‘...")
            max_sn = crawler.get_max_sn_from_sheet()
            if max_sn is None:
                print("âŒ ì‹œíŠ¸ì—ì„œ ìµœëŒ€ SNì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --since-sn ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
                return 1
            since_sn = max(0, max_sn - crawler.since_sn_buffer)
            print(f"ğŸ“Š ì‹œíŠ¸ ìµœëŒ€ SN: {max_sn}, ë²„í¼: {crawler.since_sn_buffer} â†’ since_sn: {since_sn}")
        
        print(f"ğŸš€ ì¦ë¶„ í¬ë¡¤ë§ ì‹œì‘: SN {since_sn + 1}ë¶€í„°")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        rows = crawler.crawl_incremental(
            since_sn=since_sn,
            limit=args.limit,
            headless=args.headless,
            verbose=args.verbose
        )
        
        if rows:
            output_path = crawler.save_to_csv(rows, args.output)
            print(output_path)  # íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ê²½ë¡œ ì¶œë ¥
            return 0
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 1
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())